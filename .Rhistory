# Run a multiple regression with 2 regressors
# same as the model 'model_multiple1'
summary(model_multiple1)
summary(model_multiple1)$r.squared
summary(model_multiple1)$adj.r.squared
# Run a multiple regression with 3 regressors
# same as the model 'model_multiple2'
summary(model_multiple2)
summary(model_multiple2)$r.squared
summary(model_multiple2)$adj.r.squared
# To calculate R-squared manually
y    <- wage1$wage
yhat <- fitted(model_multiple2)
ehat <- resid(model_multiple2)
SSE <- sum((yhat - mean(y))^2) # sum of squares explained or model
SSR <- sum(ehat^2) # sum of squares residual
SST <- SSE + SSR # sum of squares total
# number of observations
n <- nobs(model_multiple2)
n
# degrees of freedom for model = k = # regressors
# rank gives the number of regressors including the constant
k <- model_multiple2$rank - 1
k
# degress of freedom for residual = n - k - 1
df_SSR <- n - k - 1
# degrees of freedom total = n-1 = number of observ - 1
df_SST <- n - 1
# Formula for R-squared
R_squared <- SSE/SST
# same as R-squared = 1 - SSR/SST
R_squared
# Formula for adjusted R-squared
adj_R_squared <- 1 - (SSR/df_SSR)/(SST/df_SST)
adj_R_squared
CEOSAL1 <- read.csv("Data/CEOSAL1.csv")
CEOSAL1 %>%
select(salary, lsalary, roe, sales, lsales) %>%
head(10)
CEOSAL1 %>%
select(salary, lsalary, roe, sales, lsales) %>%
str
CEOSAL1 %>%
select(salary, lsalary, roe, sales, lsales) %>%
stargazer(type = "text")
# Linear form
model_linear <- lm(salary ~ roe + sales, CEOSAL1)
summary(model_linear)
summary(model_linear)$r.squared
summary(model_linear)$adj.r.squared
# Linear-log model
model_linear_log <- lm(salary ~ roe + lsales, CEOSAL1)
summary(model_linear_log)
summary(model_linear_log)$r.squared
summary(model_linear_log)$adj.r.squared
# Log-linear form
model_log_linear <- lm(lsalary ~ roe + sales, CEOSAL1)
summary(model_log_linear)
summary(model_log_linear)$r.squared
summary(model_log_linear)$adj.r.squared
# Log-log form
model_log_log <- lm(lsalary ~ roe + lsales, CEOSAL1)
summary(model_log_log)
summary(model_log_log)$r.squared
summary(model_log_log)$adj.r.squared
# Model for wage with female
model_no_collinearity <- lm(wage ~ educ + female, wage1)
summary(model_no_collinearity)
# Male is an exact linear function of female (perfect collinearity)
wage1 %<>% mutate(male = 1 - female)
# Model for wage with male
model_no_collinearity1 <- lm(wage ~ educ + male, wage1)
summary(model_no_collinearity1)
# Try to run regression with both female and male
model_collinearity <- lm(wage ~ educ + female + male, wage1)
summary(model_collinearity)
# Run regression with "no constant" option
model_no_constant <- lm(wage ~ 0 + educ + female + male, wage1)
summary(model_no_constant)
# Test scores example
elemapi2 <- read.csv(paste0(directory, "elemapi2.csv"))
elemapi2 <- read.csv("Data/elemapi2.csv")
elemapi2 <- read.csv("Data/elemapi2.csv")
elemapi2 %<>% select(api00, avg_ed, grad_sch, col_grad)
str(elemapi2)
stargazer(elemapi2, type = "text")
head(elemapi2, 10)
# Correlation table
elemapi2 %>%
select(-api00) %>%
na.omit %>% # remove samples with NA
cor
# Run regression, find VIF. If VIF>10 then drop the variable
(model_high_vif <- lm(api00 ~ avg_ed + grad_sch + col_grad, elemapi2))
vif(model_high_vif)
# Run regression without variable that has high VIF
(model_low_vif <- lm(api00 ~ grad_sch + col_grad, elemapi2))
vif(model_low_vif)
# Run regression without the other variables
(model_simple2 <- lm(api00 ~ avg_ed, elemapi2))
# Wage2 example
HTV <- read.csv(paste0(directory, "HTV.csv"))
HTV %<>% select(wage, educ, abil)
str(HTV)
stargazer(HTV, type = "text")
head(HTV)
# True model with educ and ability
# wage = beta0 + beta1*educ + beta2*abil + u
model_true <- lm(wage ~ educ + abil, HTV)
summary(model_true)
beta1 <- coef(model_true)["educ"]
beta1
beta2 <- coef(model_true)["abil"]
HTV <- read.csv("Data/HTV.csv")
HTV %<>% select(wage, educ, abil)
str(HTV)
stargazer(HTV, type = "text")
head(HTV)
# True model with educ and ability
# wage = beta0 + beta1*educ + beta2*abil + u
model_true <- lm(wage ~ educ + abil, HTV)
summary(model_true)
beta1 <- coef(model_true)["educ"]
beta1
beta2 <- coef(model_true)["abil"]
beta2
# Model between ability and education
# abil = delta0 + delta1*educ + v
model_abil <- lm(abil ~ educ, HTV)
summary(model_abil)
delta1 <- coef(model_abil)["educ"]
delta1
# Model where ability is omitted variable, so coefficient on educ is biased
# wage = (beta0+beta2*delta0) + (beta1+beta2*delta1)*educ +(beta2*v+u)
model_omitted <- lm(wage ~ educ, HTV)
summary(model_omitted)
beta1_biased <- coef(model_omitted)["educ"]
beta1_biased
# Calculate bias and biased coefficient
bias <- beta2*delta1
bias
beta1_biased_calculated <- beta1 + beta2*delta1
beta1_biased_calculated
# True model with educ and exper
# Same as the model 'model_multiple1'
# model_multiple1 <- lm(wage ~ educ + exper, wage1)
summary(model_multiple1)
# Model between exper and education
model_exper <- lm(exper ~ educ, wage1)
summary(model_exper)
# Model where exper is omitted, so coefficient on educ is biased
# Same as the model 'model_simple'
# model_simple <- lm(wage ~ educ, wage1)
summary(model_simple)
# True model with educ ability
# Same as the model 'model_true'
# model_true <- lm(wage ~ educ + abil, HTV)
summary(model_true)
# Model where ability is omitted variable
# model_omitted <- lm(wage ~ educ, HTV)
summary(model_omitted)
# model_multiple2 <- lm(wage ~ educ + exper + tenure, wage1)
summary(model_multiple2)
# Plotting residuals with "geom_point"
ggplot(data = wage1, mapping = aes(x = educ)) +
theme_bw() +
geom_point(mapping = aes(y = uhat)) +
geom_hline(yintercept = 0, col = 'red') + # add a horizontal line
ylab(label = "Residuals") # change y-axis label
ggplot(data = wage1, mapping = aes(x = exper)) +
theme_bw() +
geom_point(mapping = aes(y = uhat)) +
geom_hline(yintercept = 0, col = 'red') +
ylab(label = "Residuals")
# Graphs show heteroscedasticity for educ and homoscedasticity for exper
# Graphs show heteroscedasticity for educ and homoscedasticity for exper
# Graphs show heteroscedasticity for educ and homoscedasticity for exper
# Graphs show heteroscedasticity for educ and homoscedasticity for exper
# Graphs show heteroscedasticity for educ and homoscedasticity for exper
wage1 <- read.csv("Data/wage1.csv"))
wage1 <- read.csv("Data/wage1.csv")
wage1 %>%
select(wage, educ, exper, tenure, female) %>%
head(10)
wage1 %>%
select(wage, educ, exper, tenure, female) %>%
str
wage1 %>%
select(wage, educ, exper, tenure, female) %>%
stargazer(type = "text")
# Draw a histogram of variable
ggplot(data = wage1) +
theme_bw() +
geom_histogram(mapping = aes(x = wage), col = 'grey')
ggplot(data = wage1) +
theme_bw() +
geom_histogram(mapping = aes(x = lwage), col = 'grey')
# The Shapiro Wilk test for normality
# H0: normality, Ha: non-normality
shapiro.test(wage1$wage)
shapiro.test(wage1$lwage)
# Run Regression
model <- lm(wage ~ educ + exper + tenure + female, wage1)
summary(model)
# Display coefficient
(coefficient <- coef(model)["exper"])
# Display standard error
(se <- vcov(model) %>% # variance-covariance matrix
diag %>% # extract diagonals
sqrt %>% # calculate square-roots
.["exper"]) # S.E. of the regressor "exper"
# Calculate t-statistic = coefficient/standard error
(tstat <- coefficient/se)
# Degrees of freedom (n-k-1)
(df_r <- model$df.residual)
# t-critical value at 5% significance level
qt(p = 0.975, df = df_r, lower.tail = TRUE)
# H0: beta[exper] = 0;   H1: beta[exper] not equal to 0
# P-value for a two-tailed test of coefficient significance
2 * pt(q = abs(tstat), df = df_r, lower.tail = FALSE)
# H0: beta[exper]<=0;   H1: beta[exper]>0
# P-value for an upper one-tailed test of positive coefficient
pt(q = tstat, df = df_r, lower.tail = FALSE)
# H0: beta[exper]>=0;   H1: beta[exper]<0
# P-value for a lower one-tailed test of negative coefficient
pt(q = tstat, df = df_r, lower.tail = TRUE)
# Critical value at 5% significance level
qt(p = 0.975, df = df_r)
# Lower bound at 95% confidence level
coefficient - 1.96 * se
# Upper bound at 95% confidence level
coefficient + 1.96 * se
# Critical value at 10% significance level
qt(p = 0.95, df = df_r)
# Lower bound at 90% confidence level
coefficient - 1.65 * se
# Upper bound at 90% confidence level
coefficient + 1.65 * se
# H0: beta[exper]=0
# Restricted model:
#   wage = alpha0 + alpha1*educ + alpha3*tenure + alpha4*female + e
model_r1 <- lm(wage ~ educ + tenure + female, wage1)
summary(model_r1)
# SSR for the restricted model ssr_r
(ssr_r1 <- sum(resid(model_r1)^2))
# Unrestricted model:
# wage = beta0 + beta1*educ + beta2*exper + beta3*tenure + beta4*female + u
# Same as the model 'model' above
model_ur <- model
summary(model_ur)
# SSR for the unrestricted model = ssr_ur, q = number of restrictions and df_denom=n-k-1
(ssr_ur <- sum(resid(model_ur)^2))
(df_ur  <- model_ur$df.residual) # df_ur = n - k - 1
q <- 1
# Calculate F-stat using ssr_r and ssr_ur
# F-stat = ((ssr_r-ssr_ur)/q) / (ssr_ur/(n-k-1))
(F_stat <- ((ssr_r1 - ssr_ur)/q) / (ssr_ur/df_ur))
# Calculate F-critical value
qf(p = 0.95, df1 = 1, df2 = df_ur)
# p-value for F-test
(F_pvalue <- pf(q = F_stat, df1 = 1, df2 = df_ur, lower.tail = F))
# F-test for coefficient significance using R commands
linearHypothesis(model, "exper = 0")
# Above is t-test or F-test for coefficient on exper = 0
# t-test for the variable experience = 0 is not what we want to test
t.test(wage1$exper, mu = 0)
# H0: beta[exper]=0 beta[tenure]=0
# Restricted model: wage = alpha0 + alpha1*educ + alpha4*female + e
model_r2 <- lm(wage ~ educ + female, wage1)
summary(model_r2)
# SSR for the restricted model ssr_r
ssr_r2 <- sum(resid(model_r2)^2)
# Unrestricted model:
# wage = beta0 + beta1*educ + beta2*exper + beta3*tenure + beta4*female + u
# Same as the model 'model_ur'
summary(model_ur)
# SSR for the unrestricted model = ssr_ur, q = number of restrictions and
# df_denom = n-k-1
ssr_ur # residual sum of squares of the unrestricted model
df_ur # residual degrees of freedom of the unrestricted model
q <- 2
# Calculate F_stat using ssr_r and ssr_ur
# F-stat=((ssr_r-ssr_ur)/q) / (ssr_ur/(n-k-1))
(F_stat <- ((ssr_r2 - ssr_ur)/q) / (ssr_ur/df_ur))
# F-critical value
qf(p = 0.05, df1 = q, df2 = df_ur, lower.tail = F)
# p-value for F-test
(F_pvalue <- pf(q = F_stat, df1 = q, df2 = df_ur, lower.tail = F))
# F-test using R commands
linearHypothesis(model_ur, c("exper = 0", "tenure = 0"))
# H0: beta[educ]=0 beta[exper]=0 beta[tenure]=0 beta[female]=0
# Restricted model: wage = alpha0 + e
# Note that alpha0 = avg(wage)
(model_r3 <- lm(wage ~ 1, wage1))
stargazer(wage1["wage"], type = "text")
# SSR for the restricted model ssr_r
( ssr_r3 <- sum(resid(model_r3)^2) )
# Unrestricted model:
# wage = beta0 + beta1*educ + beta2*exper + beta3*tenure + beta4*female + u
summary(model_ur)
summary(model_ur)$fstatistic
# SSR for the unrestricted model = ssr_ur, q = number of restrictions and
# df_denom=n-k-1
ssr_ur
df_ur
q <- 4
# Calculate F_stat using ssr_r and ssr_ur
# F-stat=((ssr_r-ssr_ur)/q) / (ssr_ur/(n-k-1))
(F_stat <- ((ssr_r3 - ssr_ur)/q) / (ssr_ur/df_ur))
# Calculate F-statistic using R-squared (alternative way)
# F-stat = (R^2/k)/(1-R^2)/(n-k-1)
R2 <- summary(model_ur)$r.squared
k  <- model_ur$rank - 1
( F_stat1 <- (R2/k) / ((1-R2)/df_ur) )
# F-critical value
qf(p = 0.95, df1 = q, df2 = df_ur)
# p-value for F-test
(F_pvalue <- pf(q = F_stat, df1 = q, df2 = df_ur, lower.tail = F))
# F-test using R commands
linearHypothesis(model_ur, c("educ = 0", "exper = 0",
"tenure = 0", "female = 0"))
# H0: beta[exper]=0 beta[tenure]=0
# Restricted model: wage = alpha0 + alpha1*educ + alpha4*female + e
# Regress dependent variable on the restricted set of independent variables
# Same as the model 'model_r2'
summary(model_r2)
q <- 2 # 2 restrictions
# Get residuals ehat
wage1$ehat <- residuals(model_r2)
# Regress residuals ehat on all independent variables
model_ehat <- lm(ehat ~ educ + exper + tenure + female, wage1)
# LM statistic = n*(R_e)^2 = n*R-squared of above regression
n <- nobs(model_ehat)
R_e2 <- summary(model_ehat)$r.squared
(LM_stat <- n*R_e2)
# Critical value for chi-square distribution
qchisq(p = 0.95, df = q)
# P-value for chi-square distribution
pchisq(LM_stat, df = q, lower.tail = FALSE)
# If p-value < 0.05 then reject null, coefficients are jointly significant
wage1 <- read.csv("Data/wage1.csv")
# Regression with full sample
model <- lm(wage ~ educ + tenure + exper, wage1)
summary(model)
(se1 <- vcov(model) %>% diag %>% sqrt %>% .["exper"])
(n1 <- nobs(model))
# Regression with half the sample
model_half <- lm(wage ~ educ + tenure + exper,
slice(wage1, 1:(n1/2-1)))
summary(model_half)
(se2 <- vcov(model_half) %>% diag %>% sqrt %>% .["exper"])
(n2 <- nobs(model_half))
se1/se2
sqrt(n2/n1)
# These ratios are almost the same.
# These ratios are almost the same.
# As the sample size n increases, standard errors change at the rate sqrt(1/n).
# These ratios are almost the same.
# As the sample size n increases, standard errors change at the rate sqrt(1/n).
# With a larger sample size, standard errors are smaller,
wage1 <- read.csv("Data/wage1.csv")
# Regression
model_1 <- lm(wage ~ educ, wage1)
summary(model_1)
wage1 %<>% mutate(wagehat1 = fitted(model_1))
ggplot(data = wage1, mapping = aes(x = educ)) +
theme_bw() +
geom_point(mapping = aes(y = wage, col = 'wage')) +
geom_point(mapping = aes(y = wagehat1, col = 'linear prediction'))
# Regression with quadratic term
# wage = beta0 + beta1*educ + beta2*educsq + u
wage1 %<>% mutate(educsq = educ^2)
model_2 <- lm(wage ~ educ + educsq, wage1)
summary(model_2)
wage1 %<>% mutate(wagehat2 = fitted(model_2))
ggplot(data = wage1, mapping = aes(x = educ)) +
theme_bw() +
geom_point(mapping = aes(y = wage, col = 'wage')) +
geom_point(mapping = aes(y = wagehat2, col = 'linear prediction'))
# Calculate min or max point for partial effect, educ*=-beta1/2*beta2
b_2      <- coef(model_2)
b_educ   <- b_2["educ"]
b_educsq <- b_2["educsq"]
-b_educ / (2*b_educsq)
# Partial effect of educ on wage = beta1 + 2*beta2*educ
b_educ + 2*b_educsq*5
b_educ + 2*b_educsq*6.18
b_educ + 2*b_educsq*10
b_educ + 2*b_educsq*15
# Calculate partial effect at the mean
(mean_educ <- mean(wage1$educ))
(pem_educ <- b_educ + 2*b_educsq*mean_educ)
# Calculating average partial effect
pe_educ  <- b_educ + 2*b_educsq*wage1$educ
(ape_educ <- mean(pe_educ))
# Regression
model_3 <- lm(wage ~ exper, wage1)
summary(model_3)
wage1 %<>% mutate(wagehat3 = fitted(model_3))
ggplot(data = wage1, mapping = aes(x = exper)) +
theme_bw() +
geom_point(mapping = aes(y = wage, col = 'wage')) +
geom_point(mapping = aes(y = wagehat3, col = 'linear prediction'))
# Regression with quadratic term
# wage = beta0 + beta1*exper + beta2*expersq + u
# expersq is given in dataset
model_4 <- lm(wage ~ exper + expersq, wage1)
summary(model_4)
wage1 %<>% mutate(wagehat4 = fitted(model_4))
ggplot(data = wage1, mapping = aes(x = exper)) +
theme_bw() +
geom_point(mapping = aes(y = wage, col = 'wage')) +
geom_point(mapping = aes(y = wagehat4, col = 'linear prediction'))
# Calculate min or max point for partial effect, exper*=-beta1/2*beta2
b_4 <- coef(model_4)
b_exper <- b_4["exper"]
b_expersq <- b_4["expersq"]
-b_exper / (2*b_expersq)
# Partial effect of exper on wage = beta1 + 2*beta2*exper
b_exper + 2*b_expersq*10
b_exper + 2*b_expersq*20
b_exper + 2*b_expersq*24.3
b_exper + 2*b_expersq*30
# Calculating partial effect at the mean
(mean_exper <- mean(wage1$exper))
(pem_exper <- b_exper + 2*b_expersq*mean_exper)
# Calculating average partial effect
pe_exper <- b_exper + 2*b_expersq*wage1$exper
(ape_exper <-  mean(pe_exper))
# Regression
model_5 <- lm(wage ~ educ + exper + tenure, wage1)
summary(model_5)
wage1 %<>% mutate(wagehat5 = fitted(model_5))
ggplot(wage1, aes(x = educ)) +
theme_bw() +
geom_point(aes(y = wage, col = 'wage')) +
geom_point(aes(y = wagehat5, col = 'linear predictor'))
# Generate interaction term
wage1 %<>% mutate(educXexper = educ*exper,
experXtenure = exper*tenure)
# Regression with interaction term
# wage = beta0 + beta1*educ + beta2*exper + beta3*tenure + beta4*educ*exper
model_6 <- lm(wage ~ educ + exper + tenure + educXexper, wage1)
summary(model_6)
wage1 %<>% mutate(wagehat6 = fitted(model_6))
ggplot(wage1, aes(x = educ)) +
theme_bw() +
geom_point(aes(y = wage, col = 'wage')) +
geom_point(aes(y = wagehat6, col = 'linear predictor'))
# Calculate partial effect of education on wage at several levels of experience
# = beta1 + beta4*exper
b_6 <- coef(model_6)
b_educ <- b_6["educ"]
b_educexper <- b_6["educXexper"]
b_educ + b_educexper*10
b_educ + b_educexper*17
b_educ + b_educexper*30
# Regression with another interaction term
model_7 <- lm(wage ~ educ + exper + tenure + experXtenure, wage1)
wage1 %<>% mutate(wagehat7 = fitted(model_7))
ggplot(wage1, aes(x = exper)) +
theme_bw() +
geom_point(aes(y = wage, col = 'wage')) +
geom_point(aes(y = wagehat7, col = 'linear predictor'))
# CEO salary example
CEOSAL2 <- read.csv(paste0(directory, "CEOSAL2.csv"))
CEOSAL2 %<>% select(salary, sales, profits, lsalary, lsales)
str(CEOSAL2)
# CEO salary example
CEOSAL2 <- read.csv(paste0(directory, "CEOSAL2.csv"))
CEOSAL2 <- read.csv("Data/CEOSAL2.csv")
CEOSAL2 %<>% select(salary, sales, profits, lsalary, lsales)
str(CEOSAL2)
stargazer(CEOSAL2, type = "text")
# Rescale salary from thousands of dollars into dollars
CEOSAL2 %<>% mutate(salary_d = salary*1000)
# Rescale sales from millions of dollars to thousands of dollars
CEOSAL2 %<>% mutate(sales_k = sales*1000)
# Descriptive statistics
CEOSAL2 %>%
select(salary, salary_d, sales, sales_k, profits) %>%
stargazer(type = "text", digits = 0)
# Regressions with original and rescaled variables
lm(salary   ~ sales   + profits, CEOSAL2) %>% summary
lm(salary_d ~ sales   + profits, CEOSAL2) %>% summary
lm(salary   ~ sales_k + profits, CEOSAL2) %>% summary
lm(salary_d ~ sales_k + profits, CEOSAL2) %>% summary
# Level-level regression
lm(salary ~ sales + profits, CEOSAL2) %>% summary
# Log-level regression
lm(lsalary ~ sales + profits, CEOSAL2) %>% summary
# Level-log regression
lm(salary ~ lsales + profits, CEOSAL2) %>% summary
# Log-log regression
lm(lsalary ~ lsales + profits, CEOSAL2) %>% summary
# Rescale salary from thousands of dollars into dollars
CEOSAL2 %<>% mutate(lsalary_d = log(salary_d))
# Rescale sales from millions of dollars to thousands of dollars
CEOSAL2 %<>% mutate(lsales_k = log(sales_k))
# Descriptive statistics
CEOSAL2 %>%
select(lsalary, lsalary_d, lsales, lsales_k, profits) %>%
stargazer(type = "text", digits = 1)
# Regressions with original logged variables and rescaled logged variables
lm(lsalary   ~ lsales   + profits, CEOSAL2) %>% summary
lm(lsalary_d ~ lsales   + profits, CEOSAL2) %>% summary
lm(lsalary   ~ lsales_k + profits, CEOSAL2) %>% summary
lm(lsalary_d ~ lsales_k + profits, CEOSAL2) %>% summary
# When logged variables are rescaled, the coefficients do not change.
# When logged variables are rescaled, the coefficients do not change.
# When logged variables are rescaled, the coefficients do not change.
# When logged variables are rescaled, the coefficients do not change.
